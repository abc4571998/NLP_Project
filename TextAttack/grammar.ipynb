{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip3 install -q pip==22.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -qU git+https://github.com/PrithivirajDamodaran/Gramformer.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: pip in /usr/local/lib/python3.8/site-packages (22.3)\n",
      "Collecting pip\n",
      "  Using cached pip-22.3.1-py3-none-any.whl (2.1 MB)\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 22.3\n",
      "    Uninstalling pip-22.3:\n",
      "      Successfully uninstalled pip-22.3\n",
      "Successfully installed pip-22.3.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!/usr/local/bin/python3 -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-17\n"
     ]
    }
   ],
   "source": [
    "from datetime import date\n",
    "today = date.today()\n",
    "print(today)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gramformer import Gramformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /root/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt') # The NLTK tokenizer\n",
    "nltk.download('maxent_ne_chunker') # NLTK named-entity chunker\n",
    "nltk.download('words') # NLTK list of words\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gramformer] Grammar error correct/highlight model loaded..\n",
      "as it turns out, you can go home again.\n",
      "('as', 'IN')\n",
      "('it', 'PRP')\n",
      "('turns', 'VBZ')\n",
      "('out', 'RP')\n",
      "(',', ',')\n",
      "('you', 'PRP')\n",
      "('can', 'MD')\n",
      "('go', 'VB')\n",
      "('home', 'NN')\n",
      "('again', 'RB')\n",
      "('.', '.')\n",
      "단어 토큰화 : ['as', 'it', 'turns', 'out', ',', 'you', 'can', 'go', 'home', 'again', '.']\n",
      "as it turns out , you can go home again .\n",
      "품사 태깅 : [('as', 'IN'), ('it', 'PRP'), ('turns', 'VBZ'), ('out', 'RP'), (',', ','), ('you', 'PRP'), ('can', 'MD'), ('go', 'VB'), ('home', 'NN'), ('again', 'RB'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "gf = Gramformer(models=1)\n",
    "\n",
    "text = \"as it turns out , you can go home again .\"\n",
    "text2 = \"lovingly photographed in the manner of a golden book sprung to life , stuart little 2 manages sweetness largely without stickiness .\"\n",
    "text3 = \"Doesn't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\"\n",
    "text =list( gf.correct(text, max_candidates=1))[0] #그래머 체크, 결과 하나만 나오게 \n",
    "print(text)\n",
    "tokenized_sentence = word_tokenize(text)\n",
    "pos = pos_tag(tokenized_sentence)\n",
    "########not 이 있다면 먼저 제거해주자.\n",
    "answer = []\n",
    "for item in pos:\n",
    "    print(item)\n",
    "    if item[0] == \"n't\" or item[0] == \"not\":\n",
    "        del tokenized_sentence[tokenized_sentence.index(\"n't\")]\n",
    "        # answer.append(tokenized_sentence)\n",
    "        continue\n",
    "    if item[1] == \"JJ\" :\n",
    "        del tokenized_sentence[tokenized_sentence.index(item[0])]\n",
    "        continue\n",
    "print('단어 토큰화 :',tokenized_sentence)\n",
    "new = ' '.join(s for s in tokenized_sentence)\n",
    "print(new)\n",
    "print('품사 태깅 :',pos_tag(tokenized_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_examples = [\"I like red and green apple\",\n",
    "\"We want to go school\",\n",
    "\"Do you like coffee?\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S I/PRP like/VBP red/JJ and/CC green/JJ apple/NN)\n",
      "(S We/PRP want/VBP to/TO go/VB school/NN)\n",
      "(S Do/VBP you/PRP like/IN coffee/NN ?/.)\n"
     ]
    }
   ],
   "source": [
    "for item in original_examples:\n",
    "    sentence = item\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "\n",
    "    # 2. Tag parts of speech using the NLTK part-of-speech tagger.\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "\n",
    "    # 3. Extract entities from tagged sentence.\n",
    "    entities = nltk.chunk.ne_chunk(tagged)\n",
    "    print(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gramformer] Grammar error correct/highlight model loaded..\n"
     ]
    }
   ],
   "source": [
    "gf = Gramformer(models=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'one of the movies a while.'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gf.correct(\"one of the movies a while.\", max_candidates=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['seongae is pretty.']\n"
     ]
    }
   ],
   "source": [
    "a = \"seongae is dobby\"\n",
    "b = [\"seongae is pretty.\"]\n",
    "if a not in b:\n",
    "    b.append(a)\n",
    "    print(\"append\")\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I green apple', 'I like apple', 'I like green apple', 'I like green', 'We to go school', 'We want to go school', 'We want to go', 'We want to school', 'Do like coffee?', 'Do you coffee?', 'Do you like coffee?', 'Do you like?']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    " \n",
    "f = open('inputs/outputs/word_delete_output2.csv', 'r', encoding='utf-8')\n",
    "example = []\n",
    "rdr = csv.reader(f)\n",
    "for line in rdr:\n",
    "    if line[0] == \"text\":\n",
    "        continue\n",
    "    else:\n",
    "        example.append(line[0])\n",
    "f.close() \n",
    "\n",
    "print(example) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Green apple.'}\n",
      "{'I like apple.'}\n",
      "{'I like green apple.'}\n",
      "{'I like green.'}\n",
      "{'We go to school.'}\n",
      "{'We want to go to school.'}\n",
      "{'We want to go.'}\n",
      "{'We want to go to school.'}\n",
      "{'Do you like coffee?'}\n",
      "{'Do you drink coffee?'}\n",
      "{'Do you like coffee?'}\n",
      "{'Do you like it?'}\n"
     ]
    }
   ],
   "source": [
    "for item in example:\n",
    "    print(gf.correct(item, max_candidates=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'examples' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [38], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m gf\u001b[39m.\u001b[39mcorrect(examples[\u001b[39m2\u001b[39m], max_candidates\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'examples' is not defined"
     ]
    }
   ],
   "source": [
    "gf.correct(examples[2], max_candidates=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip3 install -q gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_sentence(text):\n",
    "    result = gf.correct(text, max_candidates=1)\n",
    "    return list(result)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spelling_correction_if = gr.Interface(fn = correct_sentence, \n",
    "                                    inputs = \"text\",\n",
    "                                    outputs=\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "Running on public URL: https://814de88955af0822.gradio.app\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://814de88955af0822.gradio.app\" width=\"900\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<gradio.routes.App at 0x7fef463d75e0>,\n",
       " 'http://127.0.0.1:7861/',\n",
       " 'https://814de88955af0822.gradio.app')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spelling_correction_if.launch(share=True, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
