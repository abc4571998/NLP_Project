{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
      "[nltk_data] Downloading package words to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/words.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt') # The NLTK tokenizer\n",
    "nltk.download('maxent_ne_chunker') # NLTK named-entity chunker\n",
    "nltk.download('words') # NLTK list of words\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  In/IN\n",
      "  2017/CD\n",
      "  ,/,\n",
      "  star/NN\n",
      "  quarterback/NN\n",
      "  (PERSON Tom/NNP Brady/NNP)\n",
      "  led/VBD\n",
      "  the/DT\n",
      "  (ORGANIZATION Patriots/NNP)\n",
      "  to/TO\n",
      "  the/DT\n",
      "  (ORGANIZATION Super/NNP Bowl/NNP)\n",
      "  ,/,\n",
      "  but/CC\n",
      "  lost/VBD\n",
      "  to/TO\n",
      "  the/DT\n",
      "  (ORGANIZATION Philadelphia/NNP Eagles/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "sentence = ('In 2017, star quarterback Tom Brady led the Patriots to the Super Bowl, '\n",
    "           'but lost to the Philadelphia Eagles.')\n",
    "\n",
    "# 1. Tokenize using the NLTK tokenizer.\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "\n",
    "# 2. Tag parts of speech using the NLTK part-of-speech tagger.\n",
    "tagged = nltk.pos_tag(tokens)\n",
    "\n",
    "# 3. Extract entities from tagged sentence.\n",
    "entities = nltk.chunk.ne_chunk(tagged)\n",
    "print(entities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tree('PERSON', [('Tom', 'NNP'), ('Brady', 'NNP')]), Tree('ORGANIZATION', [('Patriots', 'NNP')]), Tree('ORGANIZATION', [('Super', 'NNP'), ('Bowl', 'NNP')]), Tree('ORGANIZATION', [('Philadelphia', 'NNP'), ('Eagles', 'NNP')])]\n"
     ]
    }
   ],
   "source": [
    "named_entities = [entity for entity in entities if isinstance(entity, nltk.tree.Tree)]\n",
    "print(named_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "@functools.lru_cache(maxsize=2**14)\n",
    "def get_entities(sentence):\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "    # Setting `binary=True` makes NLTK return all of the named\n",
    "    # entities tagged as NNP instead of detailed tags like\n",
    "    #'Organization', 'Geo-Political Entity', etc.\n",
    "    entities = nltk.chunk.ne_chunk(tagged, binary=True)\n",
    "    return entities.leaves()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Because', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('weather', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('so', 'RB'),\n",
       " ('cold', 'JJ'),\n",
       " (',', ','),\n",
       " ('I', 'PRP'),\n",
       " ('ca', 'MD'),\n",
       " (\"n't\", 'RB'),\n",
       " ('go', 'VB'),\n",
       " ('picnic', 'JJ')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"Because the weather is so cold, I can't go picnic\"\n",
    "get_entities(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Sun', 'NNP'), (\"'s\", 'POS'), ('Looking', 'VBG'), ('Glass', 'NNP'), ('Provides', 'NNP'), ('3D', 'CD'), ('View', 'NNP'), ('.', '.')]\n",
      "(S\n",
      "  Sun/NNP\n",
      "  's/POS\n",
      "  Looking/VBG\n",
      "  Glass/NNP\n",
      "  Provides/NNP\n",
      "  3D/CD\n",
      "  View/NNP\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "sentence = [(\"the\", \"DT\"), (\"little\", \"JJ\"), (\"yellow\", \"JJ\"), (\"dog\", \"NN\"), (\"barked\", \"VBD\"), (\"at\", \"IN\"),  (\"the\", \"DT\"), (\"cat\", \"NN\")]\n",
    "sentence2 = \"Sun's Looking Glass Provides 3D View.\"\n",
    "print(get_entities(sentence2))\n",
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "result = cp.parse(get_entities(sentence2))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2022-10-25 04:54:13.036057: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-25 04:54:13.132442: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-10-25 04:54:13.132461: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-10-25 04:54:13.152256: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-10-25 04:54:13.665239: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-10-25 04:54:13.665309: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-10-25 04:54:13.665330: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******this is Embedding\n",
      "*******this is clare in seongae\n"
     ]
    }
   ],
   "source": [
    "from textattack.constraints import Constraint\n",
    "\n",
    "class NamedEntityConstraint(Constraint):\n",
    "    \"\"\" A constraint that ensures `transformed_text` only substitutes named entities from `current_text` with other named entities.\n",
    "    \"\"\"\n",
    "    def _check_constraint(self, transformed_text, current_text):\n",
    "        transformed_entities = get_entities(transformed_text.text)\n",
    "        current_entities = get_entities(current_text.text)\n",
    "        # If there aren't named entities, let's return False (the attack\n",
    "        # will eventually fail).\n",
    "        if len(current_entities) == 0:\n",
    "            return False\n",
    "        if len(current_entities) != len(transformed_entities):\n",
    "            # If the two sentences have a different number of entities, then \n",
    "            # they definitely don't have the same labels. In this case, the \n",
    "            # constraint is violated, and we return False.\n",
    "            return False\n",
    "        else:\n",
    "            # Here we compare all of the words, in order, to make sure that they match.\n",
    "            # If we find two words that don't match, this means a word was swapped \n",
    "            # between `current_text` and `transformed_text`. That word must be a named entity to fulfill our\n",
    "            # constraint.\n",
    "            current_word_label = None\n",
    "            transformed_word_label = None\n",
    "            for (word_1, label_1), (word_2, label_2) in zip(current_entities, transformed_entities):\n",
    "                if word_1 != word_2:\n",
    "                    # Finally, make sure that words swapped between `x` and `x_adv` are named entities. If \n",
    "                    # they're not, then we also return False.\n",
    "                    if (label_1 not in ['JJ', 'CD', 'DT']) or (label_2 not in ['JJ', 'CD', 'DT']):\n",
    "                        return False            \n",
    "            # If we get here, all of the labels match up. Return True!\n",
    "            return True\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "textattack: Unknown if model of class <class 'transformers.models.albert.modeling_albert.AlbertForSequenceClassification'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.\n",
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (/root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548)\n",
      "100%|██████████| 2/2 [00:00<00:00, 741.70it/s]\n",
      "textattack: Loading \u001b[94mdatasets\u001b[0m dataset \u001b[94mag_news\u001b[0m, split \u001b[94mtest\u001b[0m.\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from textattack.models.wrappers import HuggingFaceModelWrapper\n",
    "\n",
    "model = transformers.AutoModelForSequenceClassification.from_pretrained(\"textattack/albert-base-v2-ag-news\")\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"textattack/albert-base-v2-ag-news\")\n",
    "\n",
    "model_wrapper = HuggingFaceModelWrapper(model, tokenizer)\n",
    "\n",
    "# Create the goal function using the model\n",
    "from textattack.goal_functions import UntargetedClassification\n",
    "goal_function = UntargetedClassification(model_wrapper)\n",
    "\n",
    "# Import the dataset\n",
    "from textattack.datasets import HuggingFaceDataset\n",
    "dataset = HuggingFaceDataset(\"ag_news\", None, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 7.93 GiB total capacity; 314.70 MiB already allocated; 45.81 MiB free; 350.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 27\u001b[0m\n\u001b[1;32m     21\u001b[0m shared_tokenizer \u001b[39m=\u001b[39m transformers\u001b[39m.\u001b[39mAutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     22\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mdistilroberta-base\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     23\u001b[0m         )\n\u001b[1;32m     24\u001b[0m \u001b[39m#transformation = WordDeletion()\u001b[39;00m\n\u001b[1;32m     25\u001b[0m transformation \u001b[39m=\u001b[39m CompositeTransformation(\n\u001b[1;32m     26\u001b[0m     [\n\u001b[0;32m---> 27\u001b[0m         WordInsertionMaskedLM(\n\u001b[1;32m     28\u001b[0m             masked_language_model\u001b[39m=\u001b[39;49mshared_masked_lm,\n\u001b[1;32m     29\u001b[0m             tokenizer\u001b[39m=\u001b[39;49mshared_tokenizer,\n\u001b[1;32m     30\u001b[0m             max_candidates\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m,\n\u001b[1;32m     31\u001b[0m             min_confidence\u001b[39m=\u001b[39;49m\u001b[39m0.0\u001b[39;49m,\n\u001b[1;32m     32\u001b[0m         )\n\u001b[1;32m     33\u001b[0m     ]\n\u001b[1;32m     34\u001b[0m )\n\u001b[1;32m     36\u001b[0m \u001b[39m# We'll use the greedy search with word importance ranking method again\u001b[39;00m\n\u001b[1;32m     37\u001b[0m search_method \u001b[39m=\u001b[39m GreedySearch()\n",
      "File \u001b[0;32m~/seongae/TextAttack_copy/NLP_Project/TextAttack/textattack/transformations/word_insertions/word_insertion_masked_lm.py:68\u001b[0m, in \u001b[0;36mWordInsertionMaskedLM.__init__\u001b[0;34m(self, masked_language_model, tokenizer, max_length, window_size, max_candidates, min_confidence, batch_size)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     65\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m`tokenizer` argument must be provided when passing an actual model as `masked_language_model`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     66\u001b[0m         )\n\u001b[1;32m     67\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lm_tokenizer \u001b[39m=\u001b[39m tokenizer\n\u001b[0;32m---> 68\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_language_model\u001b[39m.\u001b[39;49mto(utils\u001b[39m.\u001b[39;49mdevice)\n\u001b[1;32m     69\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_language_model\u001b[39m.\u001b[39meval()\n\u001b[1;32m     70\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmasked_lm_name \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_language_model\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/torch/nn/modules/module.py:927\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    923\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    924\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m    925\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m--> 927\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/torch/nn/modules/module.py:579\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    578\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 579\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    581\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    582\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    583\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    584\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    589\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    590\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/torch/nn/modules/module.py:579\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    578\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 579\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    581\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    582\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    583\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    584\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    589\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    590\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/torch/nn/modules/module.py:579\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    578\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 579\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    581\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    582\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    583\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    584\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    589\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    590\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/torch/nn/modules/module.py:602\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    599\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    600\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    601\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 602\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    603\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    604\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/torch/nn/modules/module.py:925\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[1;32m    923\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    924\u001b[0m                 non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m--> 925\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(device, dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, non_blocking)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 7.93 GiB total capacity; 314.70 MiB already allocated; 45.81 MiB free; 350.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "from textattack.transformations import (\n",
    "    CompositeTransformation,\n",
    "    WordInsertionMaskedLM,\n",
    "    WordMergeMaskedLM,\n",
    "    WordSwapMaskedLM,\n",
    "    WordDeletion\n",
    ")\n",
    "from textattack.constraints.semantics.sentence_encoders import UniversalSentenceEncoder\n",
    "from textattack.goal_functions import UntargetedClassification\n",
    "\n",
    "from textattack.search_methods import GreedySearch\n",
    "from textattack import Attack\n",
    "from textattack.constraints.pre_transformation import RepeatModification, StopwordModification\n",
    "import transformers\n",
    "\n",
    "# We're going to the `WordSwapEmbedding` transformation. Using the default settings, this\n",
    "# will try substituting words with their neighbors in the counter-fitted embedding space. \n",
    "shared_masked_lm = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "            \"distilroberta-base\"\n",
    "        )\n",
    "shared_tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "            \"distilroberta-base\"\n",
    "        )\n",
    "#transformation = WordDeletion()\n",
    "transformation = WordInsertionMaskedLM(\n",
    "            masked_language_model=shared_masked_lm,\n",
    "            tokenizer=shared_tokenizer,\n",
    "            max_candidates=50,\n",
    "            min_confidence=0.0,\n",
    "        )\n",
    "    \n",
    "\n",
    "# We'll use the greedy search with word importance ranking method again\n",
    "search_method = GreedySearch()\n",
    "goal_function = UntargetedClassification(model_wrapper)\n",
    "\n",
    "# Our constraints will be the same as Tutorial 1, plus the named entity constraint\n",
    "constraints = [RepeatModification(),\n",
    "               StopwordModification()]\n",
    "use_constraint = UniversalSentenceEncoder(\n",
    "    threshold=0.7,\n",
    "    metric=\"cosine\",\n",
    "    compare_against_original=True,\n",
    "    window_size=15,\n",
    "    skip_text_shorter_than_window=True,\n",
    ")\n",
    "constraints.append(use_constraint)\n",
    "# Now, let's make the attack using these parameters. \n",
    "attack = Attack(goal_function, constraints, transformation, search_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "textattack: Logging to CSV at path results2.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack2(\n",
      "  (search_method): GreedySearch\n",
      "  (goal_function):  UntargetedClassification\n",
      "  (transformation):  CompositeTransformation(\n",
      "    (0): WordSwapMaskedLM(\n",
      "        (method):  bae\n",
      "        (masked_lm_name):  RobertaForCausalLM\n",
      "        (max_length):  512\n",
      "        (max_candidates):  50\n",
      "        (min_confidence):  0.0005\n",
      "      )\n",
      "    (1): WordInsertionMaskedLM(\n",
      "        (masked_lm_name):  RobertaForCausalLM\n",
      "        (max_length):  512\n",
      "        (max_candidates):  50\n",
      "        (min_confidence):  0.0\n",
      "      )\n",
      "    (2): WordMergeMaskedLM(\n",
      "        (masked_lm_name):  RobertaForCausalLM\n",
      "        (max_length):  512\n",
      "        (max_candidates):  50\n",
      "        (min_confidence):  0.005\n",
      "      )\n",
      "    )\n",
      "  (constraints): \n",
      "    (0): UniversalSentenceEncoder(\n",
      "        (metric):  cosine\n",
      "        (threshold):  0.7\n",
      "        (window_size):  15\n",
      "        (skip_text_shorter_than_window):  True\n",
      "        (compare_against_original):  True\n",
      "      )\n",
      "    (1): RepeatModification\n",
      "    (2): StopwordModification\n",
      "  (is_black_box):  True\n",
      ") \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/huggingface_hub/file_download.py:588: FutureWarning: `cached_download` is the legacy way to download files from the HF hub, please consider upgrading to `hf_hub_download`\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-10-25 04:56:43,494 loading file /root/.flair/models/upos-english-fast/b631371788604e95f27b6567fe7220e4a7e8d03201f3d862e6204dbf90f9f164.0afb95b43b32509bf4fcc3687f7c64157d8880d08f813124c1bd371c3d8ee3f7\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "torch.load does not work with file-like objects that do not implement readinto on Python 3.8.0 and 3.8.1. Received object of type \"<class 'mmap.mmap'>\". Please update to Python 3.8.2 or newer to restore this functionality.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m attack_args \u001b[39m=\u001b[39m AttackArgs(num_successful_examples\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, log_to_csv\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mresults2.csv\u001b[39m\u001b[39m\"\u001b[39m, csv_coloring_style\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhtml\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m attacker \u001b[39m=\u001b[39m Attacker(attack, dataset, attack_args)\n\u001b[0;32m----> 8\u001b[0m attacker\u001b[39m.\u001b[39;49mattack_dataset()\n",
      "File \u001b[0;32m~/seongae/TextAttack_copy/NLP_Project/TextAttack/textattack/attacker.py:441\u001b[0m, in \u001b[0;36mAttacker.attack_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_attack_parallel()\n\u001b[1;32m    440\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 441\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_attack()\n\u001b[1;32m    443\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattack_args\u001b[39m.\u001b[39msilent:\n\u001b[1;32m    444\u001b[0m     logger\u001b[39m.\u001b[39msetLevel(logging\u001b[39m.\u001b[39mINFO)\n",
      "File \u001b[0;32m~/seongae/TextAttack_copy/NLP_Project/TextAttack/textattack/attacker.py:170\u001b[0m, in \u001b[0;36mAttacker._attack\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    168\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattack\u001b[39m.\u001b[39mattack(example, ground_truth_output)\n\u001b[1;32m    169\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 170\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    171\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    172\u001b[0m     \u001b[39misinstance\u001b[39m(result, SkippedAttackResult) \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattack_args\u001b[39m.\u001b[39mattack_n\n\u001b[1;32m    173\u001b[0m ) \u001b[39mor\u001b[39;00m (\n\u001b[1;32m    174\u001b[0m     \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(result, SuccessfulAttackResult)\n\u001b[1;32m    175\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattack_args\u001b[39m.\u001b[39mnum_successful_examples\n\u001b[1;32m    176\u001b[0m ):\n\u001b[1;32m    177\u001b[0m     \u001b[39mif\u001b[39;00m worklist_candidates:\n",
      "File \u001b[0;32m~/seongae/TextAttack_copy/NLP_Project/TextAttack/textattack/attacker.py:168\u001b[0m, in \u001b[0;36mAttacker._attack\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    166\u001b[0m     example\u001b[39m.\u001b[39mattack_attrs[\u001b[39m\"\u001b[39m\u001b[39mlabel_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39mlabel_names\n\u001b[1;32m    167\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 168\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattack\u001b[39m.\u001b[39;49mattack(example, ground_truth_output)\n\u001b[1;32m    169\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    170\u001b[0m     \u001b[39mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/seongae/TextAttack_copy/NLP_Project/TextAttack/textattack/attack.py:448\u001b[0m, in \u001b[0;36mAttack.attack\u001b[0;34m(self, example, ground_truth_output)\u001b[0m\n\u001b[1;32m    446\u001b[0m     \u001b[39mreturn\u001b[39;00m SkippedAttackResult(goal_function_result)\n\u001b[1;32m    447\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 448\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_attack(goal_function_result)\n\u001b[1;32m    449\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/seongae/TextAttack_copy/NLP_Project/TextAttack/textattack/attack.py:396\u001b[0m, in \u001b[0;36mAttack._attack\u001b[0;34m(self, initial_result)\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_attack\u001b[39m(\u001b[39mself\u001b[39m, initial_result):\n\u001b[1;32m    386\u001b[0m     \u001b[39m\"\"\"Calls the ``SearchMethod`` to perturb the ``AttackedText`` stored in\u001b[39;00m\n\u001b[1;32m    387\u001b[0m \u001b[39m    ``initial_result``.\u001b[39;00m\n\u001b[1;32m    388\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[39m            or ``MaximizedAttackResult``.\u001b[39;00m\n\u001b[1;32m    395\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 396\u001b[0m     final_result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msearch_method(initial_result)\n\u001b[1;32m    397\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclear_cache()\n\u001b[1;32m    398\u001b[0m     \u001b[39mif\u001b[39;00m final_result\u001b[39m.\u001b[39mgoal_status \u001b[39m==\u001b[39m GoalFunctionResultStatus\u001b[39m.\u001b[39mSUCCEEDED:\n",
      "File \u001b[0;32m~/seongae/TextAttack_copy/NLP_Project/TextAttack/textattack/search_methods/search_method.py:36\u001b[0m, in \u001b[0;36mSearchMethod.__call__\u001b[0;34m(self, initial_result)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mfilter_transformations\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     32\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\n\u001b[1;32m     33\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mSearch Method must have access to filter_transformations method\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     34\u001b[0m     )\n\u001b[0;32m---> 36\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mperform_search(initial_result)\n\u001b[1;32m     37\u001b[0m \u001b[39m# ensure that the number of queries for this GoalFunctionResult is up-to-date\u001b[39;00m\n\u001b[1;32m     38\u001b[0m result\u001b[39m.\u001b[39mnum_queries \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgoal_function\u001b[39m.\u001b[39mnum_queries\n",
      "File \u001b[0;32m~/seongae/TextAttack_copy/NLP_Project/TextAttack/textattack/search_methods/beam_search.py:32\u001b[0m, in \u001b[0;36mBeamSearch.perform_search\u001b[0;34m(self, initial_result)\u001b[0m\n\u001b[1;32m     30\u001b[0m potential_next_beam \u001b[39m=\u001b[39m []\n\u001b[1;32m     31\u001b[0m \u001b[39mfor\u001b[39;00m text \u001b[39min\u001b[39;00m beam:\n\u001b[0;32m---> 32\u001b[0m     transformations \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_transformations(\n\u001b[1;32m     33\u001b[0m         text, original_text\u001b[39m=\u001b[39;49minitial_result\u001b[39m.\u001b[39;49mattacked_text\n\u001b[1;32m     34\u001b[0m     )\n\u001b[1;32m     35\u001b[0m     potential_next_beam \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m transformations\n\u001b[1;32m     37\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(potential_next_beam) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     38\u001b[0m     \u001b[39m# If we did not find any possible perturbations, give up.\u001b[39;00m\n",
      "File \u001b[0;32m~/seongae/TextAttack_copy/NLP_Project/TextAttack/textattack/attack.py:303\u001b[0m, in \u001b[0;36mAttack.get_transformations\u001b[0;34m(self, current_text, original_text, **kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m     transformed_texts \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformation_cache[cache_key])\n\u001b[1;32m    302\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 303\u001b[0m     transformed_texts \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_transformations_uncached(\n\u001b[1;32m    304\u001b[0m         current_text, original_text, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    305\u001b[0m     )\n\u001b[1;32m    306\u001b[0m     \u001b[39mif\u001b[39;00m utils\u001b[39m.\u001b[39mhashable(cache_key):\n\u001b[1;32m    307\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformation_cache[cache_key] \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(transformed_texts)\n",
      "File \u001b[0;32m~/seongae/TextAttack_copy/NLP_Project/TextAttack/textattack/attack.py:271\u001b[0m, in \u001b[0;36mAttack._get_transformations_uncached\u001b[0;34m(self, current_text, original_text, **kwargs)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_transformations_uncached\u001b[39m(\u001b[39mself\u001b[39m, current_text, original_text\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    262\u001b[0m     \u001b[39m\"\"\"Applies ``self.transformation`` to ``text``, then filters the list\u001b[39;00m\n\u001b[1;32m    263\u001b[0m \u001b[39m    of possible transformations through the applicable constraints.\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[39m        A filtered list of transformations where each transformation matches the constraints\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 271\u001b[0m     transformed_texts \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformation(\n\u001b[1;32m    272\u001b[0m         current_text,\n\u001b[1;32m    273\u001b[0m         pre_transformation_constraints\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpre_transformation_constraints,\n\u001b[1;32m    274\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    275\u001b[0m     )\n\u001b[1;32m    277\u001b[0m     \u001b[39mreturn\u001b[39;00m transformed_texts\n",
      "File \u001b[0;32m~/seongae/TextAttack_copy/NLP_Project/TextAttack/textattack/transformations/composite_transformation.py:39\u001b[0m, in \u001b[0;36mCompositeTransformation.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     37\u001b[0m new_attacked_texts \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n\u001b[1;32m     38\u001b[0m \u001b[39mfor\u001b[39;00m transformation \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformations:\n\u001b[0;32m---> 39\u001b[0m     new_attacked_texts\u001b[39m.\u001b[39mupdate(transformation(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs))\n\u001b[1;32m     40\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(new_attacked_texts)\n",
      "File \u001b[0;32m~/seongae/TextAttack_copy/NLP_Project/TextAttack/textattack/transformations/transformation.py:57\u001b[0m, in \u001b[0;36mTransformation.__call__\u001b[0;34m(self, current_text, pre_transformation_constraints, indices_to_modify, shifted_idxs, return_indices)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[39mif\u001b[39;00m return_indices:\n\u001b[1;32m     55\u001b[0m     \u001b[39mreturn\u001b[39;00m indices_to_modify\n\u001b[0;32m---> 57\u001b[0m transformed_texts \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_transformations(current_text, indices_to_modify)\n\u001b[1;32m     58\u001b[0m \u001b[39mfor\u001b[39;00m text \u001b[39min\u001b[39;00m transformed_texts:\n\u001b[1;32m     59\u001b[0m     text\u001b[39m.\u001b[39mattack_attrs[\u001b[39m\"\u001b[39m\u001b[39mlast_transformation\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/seongae/TextAttack_copy/NLP_Project/TextAttack/textattack/transformations/word_merges/word_merge_masked_lm.py:162\u001b[0m, in \u001b[0;36mWordMergeMaskedLM._get_transformations\u001b[0;34m(self, current_text, indices_to_modify)\u001b[0m\n\u001b[1;32m    160\u001b[0m indices_to_modify \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(indices_to_modify)\n\u001b[1;32m    161\u001b[0m \u001b[39m# find indices that are suitable to merge\u001b[39;00m\n\u001b[0;32m--> 162\u001b[0m token_tags \u001b[39m=\u001b[39m [\n\u001b[1;32m    163\u001b[0m     current_text\u001b[39m.\u001b[39mpos_of_word_index(i) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(current_text\u001b[39m.\u001b[39mnum_words)\n\u001b[1;32m    164\u001b[0m ]\n\u001b[1;32m    165\u001b[0m merge_indices \u001b[39m=\u001b[39m find_merge_index(token_tags)\n\u001b[1;32m    166\u001b[0m merged_words \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_merged_words(current_text, merge_indices)\n",
      "File \u001b[0;32m~/seongae/TextAttack_copy/NLP_Project/TextAttack/textattack/transformations/word_merges/word_merge_masked_lm.py:163\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    160\u001b[0m indices_to_modify \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(indices_to_modify)\n\u001b[1;32m    161\u001b[0m \u001b[39m# find indices that are suitable to merge\u001b[39;00m\n\u001b[1;32m    162\u001b[0m token_tags \u001b[39m=\u001b[39m [\n\u001b[0;32m--> 163\u001b[0m     current_text\u001b[39m.\u001b[39;49mpos_of_word_index(i) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(current_text\u001b[39m.\u001b[39mnum_words)\n\u001b[1;32m    164\u001b[0m ]\n\u001b[1;32m    165\u001b[0m merge_indices \u001b[39m=\u001b[39m find_merge_index(token_tags)\n\u001b[1;32m    166\u001b[0m merged_words \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_merged_words(current_text, merge_indices)\n",
      "File \u001b[0;32m~/seongae/TextAttack_copy/NLP_Project/TextAttack/textattack/shared/attacked_text.py:148\u001b[0m, in \u001b[0;36mAttackedText.pos_of_word_index\u001b[0;34m(self, desired_word_idx)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pos_tags:\n\u001b[1;32m    144\u001b[0m     sentence \u001b[39m=\u001b[39m Sentence(\n\u001b[1;32m    145\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtext,\n\u001b[1;32m    146\u001b[0m         use_tokenizer\u001b[39m=\u001b[39mtextattack\u001b[39m.\u001b[39mshared\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mTextAttackFlairTokenizer(),\n\u001b[1;32m    147\u001b[0m     )\n\u001b[0;32m--> 148\u001b[0m     textattack\u001b[39m.\u001b[39;49mshared\u001b[39m.\u001b[39;49mutils\u001b[39m.\u001b[39;49mflair_tag(sentence)\n\u001b[1;32m    149\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pos_tags \u001b[39m=\u001b[39m sentence\n\u001b[1;32m    150\u001b[0m flair_word_list, flair_pos_list \u001b[39m=\u001b[39m textattack\u001b[39m.\u001b[39mshared\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mzip_flair_result(\n\u001b[1;32m    151\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pos_tags\n\u001b[1;32m    152\u001b[0m )\n",
      "File \u001b[0;32m~/seongae/TextAttack_copy/NLP_Project/TextAttack/textattack/shared/utils/strings.py:229\u001b[0m, in \u001b[0;36mflair_tag\u001b[0;34m(sentence, tag_type)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _flair_pos_tagger:\n\u001b[1;32m    227\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mflair\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m SequenceTagger\n\u001b[0;32m--> 229\u001b[0m     _flair_pos_tagger \u001b[39m=\u001b[39m SequenceTagger\u001b[39m.\u001b[39;49mload(tag_type)\n\u001b[1;32m    230\u001b[0m _flair_pos_tagger\u001b[39m.\u001b[39mpredict(sentence, force_token_predictions\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/flair/nn/model.py:142\u001b[0m, in \u001b[0;36mModel.load\u001b[0;34m(cls, model_path)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[39m# load_big_file is a workaround byhttps://github.com/highway11git\u001b[39;00m\n\u001b[1;32m    139\u001b[0m     \u001b[39m# to load models on some Mac/Windows setups\u001b[39;00m\n\u001b[1;32m    140\u001b[0m     \u001b[39m# see https://github.com/zalandoresearch/flair/issues/351\u001b[39;00m\n\u001b[1;32m    141\u001b[0m     f \u001b[39m=\u001b[39m file_utils\u001b[39m.\u001b[39mload_big_file(\u001b[39mstr\u001b[39m(model_file))\n\u001b[0;32m--> 142\u001b[0m     state \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mload(f, map_location\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    144\u001b[0m model \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_init_model_with_state_dict(state)\n\u001b[1;32m    146\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mmodel_card\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m state:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/torch/serialization.py:713\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    711\u001b[0m             \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mload(opened_file)\n\u001b[1;32m    712\u001b[0m         \u001b[39mreturn\u001b[39;00m _load(opened_zipfile, map_location, pickle_module, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[0;32m--> 713\u001b[0m \u001b[39mreturn\u001b[39;00m _legacy_load(opened_file, map_location, pickle_module, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpickle_load_args)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/torch/serialization.py:915\u001b[0m, in \u001b[0;36m_legacy_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    912\u001b[0m         f\u001b[39m.\u001b[39mseek(\u001b[39m0\u001b[39m)\n\u001b[1;32m    914\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(f, \u001b[39m'\u001b[39m\u001b[39mreadinto\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mand\u001b[39;00m (\u001b[39m3\u001b[39m, \u001b[39m8\u001b[39m, \u001b[39m0\u001b[39m) \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m sys\u001b[39m.\u001b[39mversion_info \u001b[39m<\u001b[39m (\u001b[39m3\u001b[39m, \u001b[39m8\u001b[39m, \u001b[39m2\u001b[39m):\n\u001b[0;32m--> 915\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    916\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtorch.load does not work with file-like objects that do not implement readinto on Python 3.8.0 and 3.8.1. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    917\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mReceived object of type \u001b[39m\u001b[39m\\\"\u001b[39;00m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(f)\u001b[39m}\u001b[39;00m\u001b[39m\\\"\u001b[39;00m\u001b[39m. Please update to Python 3.8.2 or newer to restore this \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    918\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mfunctionality.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    920\u001b[0m magic_number \u001b[39m=\u001b[39m pickle_module\u001b[39m.\u001b[39mload(f, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[1;32m    921\u001b[0m \u001b[39mif\u001b[39;00m magic_number \u001b[39m!=\u001b[39m MAGIC_NUMBER:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: torch.load does not work with file-like objects that do not implement readinto on Python 3.8.0 and 3.8.1. Received object of type \"<class 'mmap.mmap'>\". Please update to Python 3.8.2 or newer to restore this functionality."
     ]
    }
   ],
   "source": [
    "from textattack.loggers import CSVLogger # tracks a dataframe for us.\n",
    "from textattack.attack_results import SuccessfulAttackResult\n",
    "from textattack import Attacker, AttackArgs\n",
    "\n",
    "attack_args = AttackArgs(num_successful_examples=5, log_to_csv=\"results2.csv\", csv_coloring_style=\"html\")\n",
    "attacker = Attacker(attack, dataset, attack_args)\n",
    "\n",
    "attacker.attack_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29018/2781339431.py:4: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_text</th>\n",
       "      <th>perturbed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fears for T N pension after talks Unions representing workers at <font color = blue>Turner</font>   Newall say they are 'disappointed' after talks with stricken parent firm Federal <font color = blue>Mogul</font>.</td>\n",
       "      <td>Fears for T N pension after talks Unions representing workers at <font color = purple>Knapp</font>   Newall say they are 'disappointed' after talks with stricken parent firm Federal <font color = purple>Titan</font>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td><font color = purple>Prediction</font> <font color = purple>Unit</font> Helps <font color = purple>Forecast</font> Wildfires (AP) AP - It's barely dawn when <font color = purple>Mike</font> Fitzpatrick starts his shift with a blur of colorful maps, figures and endless charts, but already he knows what the day will bring. Lightning will strike in places he expects. Winds will pick up, moist places will dry and flames will roar.</td>\n",
       "      <td><font color = green>Foresight</font> <font color = green>Driving</font> Helps <font color = green>Expectations</font> Wildfires (AP) AP - It's barely dawn when <font color = green>Meek</font> Fitzpatrick starts his shift with a blur of colorful maps, figures and endless charts, but already he knows what the day will bring. Lightning will strike in places he expects. Winds will pick up, moist places will dry and flames will roar.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>E-mail scam targets police chief Wiltshire <font color = purple>Police</font> warns about \"phishing\" after its fraud squad chief was targeted.</td>\n",
       "      <td>E-mail scam targets police chief Wiltshire <font color = red>Constabulary</font> warns about \"phishing\" after its fraud squad chief was targeted.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Apple <font color = purple>Launches</font> <font color = purple>Graphics</font> Software, <font color = purple>Video</font> <font color = purple>Bundle</font>  LOS <font color = purple>ANGELES</font> (<font color = purple>Reuters</font>) - Apple <font color = purple>Computer</font> Inc.&lt;AAPL.<font color = purple>O</font>&gt; on  <font color = purple>Tuesday</font> began shipping a new program designed to let users  create real-time motion graphics and unveiled a discount  video-editing software bundle featuring its flagship <font color = purple>Final</font> <font color = purple>Cut</font>  <font color = purple>Pro</font> software.</td>\n",
       "      <td>Apple <font color = blue>Startup</font> <font color = blue>Charting</font> Software, <font color = blue>Film</font> <font color = blue>Pooling</font>  LOS <font color = blue>FRESNO</font> (<font color = blue>Msnbc</font>) - Apple <font color = blue>Team</font> Inc.&lt;AAPL.<font color = blue>s</font>&gt; on  <font color = blue>Friday</font> began shipping a new program designed to let users  create real-time motion graphics and unveiled a discount  video-editing software bundle featuring its flagship <font color = blue>Conclude</font> <font color = blue>Cuts</font>  <font color = blue>Careers</font> software.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Dutch Retailer Beats <font color = purple>Apple</font> to Local <font color = purple>Download</font> Market  AMSTERDAM (Reuters) - Free <font color = purple>Record</font> <font color = purple>Shop</font>, a Dutch music  retail chain, beat <font color = purple>Apple</font> <font color = purple>Computer</font> Inc. to market on Tuesday  with the launch of a new download service in Europe's latest  battleground for digital song services.</td>\n",
       "      <td>Dutch Retailer Beats <font color = blue>Abel</font> to Local <font color = blue>Absolution</font> Market  AMSTERDAM (Reuters) - Free <font color = blue>Registering</font> <font color = blue>Depot</font>, a Dutch music  retail chain, beat <font color = blue>Cobbler</font> <font color = blue>Typewriters</font> Inc. to market on Tuesday  with the launch of a new download service in Europe's latest  battleground for digital song services.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_colwidth = 480 # increase column width so we can actually read the examples\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "logger = attacker.attack_log_manager.loggers[0]\n",
    "successes = logger.df[logger.df[\"result_type\"] == \"Successful\"]\n",
    "display(HTML(successes[['original_text', 'perturbed_text']].to_html(escape=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "python3.8 is already the newest version (3.8.0-3ubuntu1~18.04.2).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.\n",
      "2 not fully installed or removed.\n",
      "After this operation, 0 B of additional disk space will be used.\n",
      "Do you want to continue? [Y/n] ^C\n"
     ]
    }
   ],
   "source": [
    "!sudo apt-get install python3.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gramformer in /usr/local/lib/python3.8/site-packages (1.0)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.8/site-packages (from gramformer) (2022.8.2)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.8/site-packages (from gramformer) (0.1.95)\n",
      "Requirement already satisfied: tokenizers in /usr/local/lib/python3.8/site-packages (from gramformer) (0.12.1)\n",
      "Requirement already satisfied: python-Levenshtein in /usr/local/lib/python3.8/site-packages (from gramformer) (0.20.7)\n",
      "Requirement already satisfied: errant in /usr/local/lib/python3.8/site-packages (from gramformer) (2.3.3)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.8/site-packages (from gramformer) (4.23.1)\n",
      "Requirement already satisfied: fuzzywuzzy in /usr/local/lib/python3.8/site-packages (from gramformer) (0.18.0)\n",
      "Requirement already satisfied: spacy<3,>=2.2.0 in /usr/local/lib/python3.8/site-packages (from errant->gramformer) (2.3.7)\n",
      "Requirement already satisfied: rapidfuzz>=2.0.0 in /usr/local/lib/python3.8/site-packages (from errant->gramformer) (2.11.1)\n",
      "Requirement already satisfied: Levenshtein==0.20.7 in /usr/local/lib/python3.8/site-packages (from python-Levenshtein->gramformer) (0.20.7)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/site-packages (from transformers->gramformer) (2.28.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/site-packages (from transformers->gramformer) (6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /root/.local/lib/python3.8/site-packages (from transformers->gramformer) (21.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.8/site-packages (from transformers->gramformer) (0.10.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/site-packages (from transformers->gramformer) (2022.9.13)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/site-packages (from transformers->gramformer) (1.23.4)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/site-packages (from transformers->gramformer) (4.64.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/site-packages (from transformers->gramformer) (3.8.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers->gramformer) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /root/.local/lib/python3.8/site-packages (from packaging>=20.0->transformers->gramformer) (3.0.9)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.8/site-packages (from spacy<3,>=2.2.0->errant->gramformer) (0.7.8)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/site-packages (from spacy<3,>=2.2.0->errant->gramformer) (41.2.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/site-packages (from spacy<3,>=2.2.0->errant->gramformer) (3.0.7)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.8/site-packages (from spacy<3,>=2.2.0->errant->gramformer) (0.10.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/site-packages (from spacy<3,>=2.2.0->errant->gramformer) (1.0.8)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.8/site-packages (from spacy<3,>=2.2.0->errant->gramformer) (1.0.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.8/site-packages (from spacy<3,>=2.2.0->errant->gramformer) (1.0.5)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/site-packages (from spacy<3,>=2.2.0->errant->gramformer) (2.0.6)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.8/site-packages (from spacy<3,>=2.2.0->errant->gramformer) (1.1.3)\n",
      "Requirement already satisfied: thinc<7.5.0,>=7.4.1 in /usr/local/lib/python3.8/site-packages (from spacy<3,>=2.2.0->errant->gramformer) (7.4.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/site-packages (from requests->transformers->gramformer) (2022.9.24)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.8/site-packages (from requests->transformers->gramformer) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/site-packages (from requests->transformers->gramformer) (1.26.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/site-packages (from requests->transformers->gramformer) (3.4)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip3 install gramformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
